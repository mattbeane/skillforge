# Issue Key: Surgery Dataset Capstone

**For evaluator use only. Do not share with students before assessment.**

---

## Critical Issues (Must Catch)

### C1: Genre Violation - Hypo-Deductive Framing [Domain 5]

**Location:** Abstract, Introduction, entire paper structure

**Problem:** The paper uses hypothesis-testing framing ("We develop and test," "we hypothesize," "H1 supported") for what was actually discovery research. The research was ethnographic and inductive—mechanisms were discovered through observation, not predicted in advance.

**Evidence:**
- Abstract: "We develop and test a theory"
- Abstract: "we hypothesize that"
- Intro: "we predict that technology-mediated work will reduce..."
- Intro: "Our data confirm this prediction"
- Theory section titled "Theoretical Framework and Hypotheses"
- Numbered hypotheses (H1, H2, H3)
- Findings: "H1 Supported," "H2 Supported," "H3 Supported"

**Why critical:** This is a fundamental genre mismatch. Reviewers at ASQ or Organization Science would immediately recognize the disconnect between ethnographic methods and deductive claims. Desk rejection likely.

---

### C2: Overclaimed Contribution [Domain 2]

**Location:** Introduction, paragraph 3; Discussion

**Problem:** Claims to provide "the first systematic documentation of learning barriers in robotic surgery" and to "test and extend organizational learning theory." Neither is accurate—the paper describes known phenomena (participation barriers) and doesn't actually test theory in any rigorous sense.

**Evidence:**
- "we test and extend organizational learning theory to technology-mediated contexts"
- "the first systematic documentation" (prior work exists)
- "Our findings confirm and extend organizational learning theory"

**Why critical:** Contribution claims must be defensible. Reviewers familiar with the literature would challenge both novelty and the "test" claim.

---

### C3: Key Mechanism Undersold/Misidentified [Domain 3]

**Location:** Finding 2; entire paper

**Problem:** The paper identifies "informal learning strategies" but misses the deeper mechanism: **shadow learning**. The three strategies identified (simulation, video, opportunistic practice) are not just "informal"—they are norm-challenging practices that the community tacitly tolerates because the alternative (incompetent trainees) is worse. This is the core theoretical insight from the data, and the AI draft treats it as a minor finding.

**Evidence:**
- "Don't tell anyone I said this" quote is presented but not theorized
- No discussion of norm violation or tacit tolerance
- Shadow learning practices framed as simple "compensation" rather than institutional workaround

**Why critical:** Missing the mechanism means missing the contribution. The paper is describing the surface rather than the underlying dynamic.

---

### C4: Disconfirming Evidence Ignored [Domain 6]

**Location:** Throughout; notably absent from Discussion

**Problem:** No discussion of disconfirming evidence or boundary conditions. The data includes residents who got robotic experience through legitimate means (dual-console training), attendings who prioritized teaching over efficiency, and cases where workarounds failed. None of this is addressed.

**Evidence:**
- No "alternative explanations" section
- No discussion of cases that don't fit the pattern
- No boundary conditions for when the mechanisms operate

**Why critical:** Discovery papers must engage with disconfirming evidence. Its absence signals either incomplete analysis or intellectual dishonesty.

---

### C5: Claims Exceed Evidence [Domain 7]

**Location:** Finding 1; Quantitative claim

**Problem:** Claims "residents performed hands-on surgical work for an average of 78% of procedure time [in open surgery]... In robotic procedures, this dropped to approximately 8%." This implies systematic measurement, but the methods describe only qualitative observation and interviews. Where did these percentages come from?

**Evidence:**
- "Quantitative analysis of our observation data confirmed this pattern"
- But methods section describes only ethnographic observation and interviews
- No systematic time-on-task measurement described

**Why critical:** Fabricated or unsupported quantitative claims destroy credibility. Reviewers would immediately flag this.

---

## Major Issues (Should Catch Most)

### M1: Wrong Literature Positioning [Domain 4]

**Location:** Theoretical Framework

**Problem:** The paper positions against generic organizational learning (March, Levitt & March) and legitimate peripheral participation (Lave & Wenger) but misses more directly relevant literature on:
- Technology and skill degradation
- Surgical training specifically
- Norm-challenging practices (Anteby, Bernstein)
- Communities of practice breakdown

**Why major:** Better positioning would sharpen the contribution. Current framing is too generic.

---

### M2: Mechanism 3 Misframed [Domain 3]

**Location:** Finding 3; "Workarounds Maintain Performance"

**Problem:** The workaround finding is real but the framing misses the portfolio degradation dynamic. It's not just that staff develop workarounds—it's that workarounds *mask* problems, preventing management from seeing the need to invest. The "venting cycle" is identified but not theorized as a mechanism of organizational stasis.

**Why major:** Partial mechanism identification. The pattern is seen but not fully understood.

---

### M3: Sample Description Mismatch [Domain 7]

**Location:** Methods

**Problem:** Claims 47 procedures and 23 interviews, but the data shown doesn't clearly derive from this sample. Expert baseline shows 69 quotations from specific dates/procedures not mentioned in methods. The numbers feel invented.

**Why major:** Methods-data mismatch undermines trustworthiness.

---

### M4: Missing "How I Found It" [Domain 5]

**Location:** Methods, throughout

**Problem:** No discussion of analytical iteration or discovery process. Paper reads as if hypotheses were formed first and data confirmed them—but ethnographic research doesn't work that way. Where's the "I was surprised by..." or "Initial analysis suggested X but further examination revealed Y"?

**Why major:** Temporal logic of discovery is missing. Paper reads as retrofitted.

---

### M5: Technology Friction Undertheorized [Domain 3]

**Location:** Finding 3

**Problem:** The "shitty robot" dynamic (older robot with worn equipment, software crashes, jerry-rigging) is mentioned but not connected to the participation barrier story. These are related: the older robot gets worse cases AND worse equipment, creating compounding disadvantage. The AI draft treats them as separate findings.

**Why major:** Missed integration across findings.

---

## Minor Issues (Good to Catch)

### m1: Generic Abstract

**Location:** Abstract

**Problem:** Abstract could describe any technology/learning study. Nothing surgical-specific until the last sentence. Should lead with the puzzle/phenomenon.

---

### m2: Weak Opening

**Location:** Introduction, first paragraph

**Problem:** "Technology transforms work" is a generic opener. Should start with the specific phenomenon—something concrete about robotic surgery and learning.

---

### m3: References Missing Key Works

**Location:** References

**Problem:** No Edmondson (surgical teams), no Beane (prior work on this topic), no Anteby or Bernstein (norm-challenging). Citation list is generic org theory.

---

### m4: "Significant" Language

**Location:** Finding 2

**Problem:** "Residents spent significant personal time on simulators"—"significant" implies statistical testing. Should be "substantial" or give actual estimates.

---

### m5: Limitations Section Pro Forma

**Location:** Discussion > Limitations

**Problem:** Standard boilerplate ("sample limited to one hospital") rather than engagement with actual study limitations (can't measure skill acquisition directly, reliance on self-report for shadow learning practices).

---

## Non-Issues (Should NOT Flag)

### NI1: Use of "Workaround" Term

The term "workaround" is used appropriately and consistently. Some students may flag this as jargon, but it's established in the literature (Alter, 2014; Orr, 1996).

---

### NI2: Quote Selection

The quotes used are appropriate illustrations of the patterns described. Some students may want different quotes, but the selections are defensible.

---

### NI3: Three-Finding Structure

Organizing findings into three themes is a reasonable choice. Some students may prefer a different structure, but this is stylistic, not substantive.

---

### NI4: Word Count

The draft is abbreviated for assessment purposes. Students should not flag length as an issue.

---

## Scoring Guide

### Critical Issues (25 points)
- C1 (Genre): 5 points
- C2 (Contribution): 5 points
- C3 (Mechanism): 5 points
- C4 (Disconfirming): 5 points
- C5 (Claims-Evidence): 5 points

**Must find at least 4 of 5 (20 points) to pass Part A**

### Major Issues (15 points)
- 3 points each; must find at least 3 of 5

### Minor Issues (5 points)
- 1 point each; credit for any found

### False Positives (-2 points each)
- Deduct for flagging non-issues
- Maximum deduction: 6 points

---

## Domain Distribution

| Domain | Critical | Major | Minor |
|--------|----------|-------|-------|
| 1. Pattern Recognition | - | - | - |
| 2. Theoretical Positioning | C2 | M1 | - |
| 3. Qualitative Mechanism | C3 | M2, M5 | - |
| 4. Theoretical Framing | - | M1 | m2 |
| 5. Epistemological Genre | C1 | M4 | m1 |
| 6. Adversarial Evidence | C4 | - | - |
| 7. Claim Verification | C5 | M3 | m3, m4, m5 |

**Note:** Pattern Recognition (Domain 1) is not directly tested—the AI "found" patterns, and the test is whether students can evaluate the AI's interpretation. Domain 1 competence is implicit in catching mechanism and evidence issues.
