# Domain 1: Pattern Recognition
## Level 1 Assessment - Knowledge Recognition

**Time**: 30-45 minutes
**Passing**: ≥80% (10/12 questions)
**Retake**: Immediate, unlimited

---

## Section A: Multiple Choice (10 questions)

### Question 1
A researcher observes that robotic surgery adoption varies significantly across hospitals (F = 3.8, p < 0.01). Which next step shows good pattern recognition instinct?

- a) Write up the finding as evidence of organizational heterogeneity in technology adoption
- b) Look for what differs between high and low adoption hospitals—the explanation IS the story
- c) Run a meta-analysis of prior studies on robotic surgery
- d) Search for theories that predict variation in technology adoption

**Correct**: b
**Why**: The heterogeneity itself isn't interesting; understanding WHY it varies is where the paper lives.

---

### Question 2
You find that surgical trainees get significantly less hands-on time in robotic procedures than open procedures (5% vs 85% of procedure time). What should you check FIRST?

- a) Whether this finding aligns with learning theory
- b) Whether this pattern holds across different types of procedures and hospitals
- c) How to frame this for a top journal
- d) What other researchers have found about surgical training

**Correct**: b
**Why**: Robustness first. If the pattern only holds in one hospital or one procedure type, the story changes.

---

### Question 3
Your data shows that experienced surgeons spend MORE time on older robots while newer surgeons get the newer equipment. You expected the opposite. What does this pattern suggest?

- a) The data is probably wrong—this violates expectations
- b) Nothing interesting—just organizational politics
- c) This anomaly could be the paper—investigate why experienced talent gets assigned to degrading technology
- d) Report this as a limitation of the data

**Correct**: c
**Why**: Patterns that violate expectations are often the most theoretically interesting. The "resourcing a technological portfolio" question emerges from exactly this anomaly.

---

### Question 4
A researcher finds that training outcomes correlate with console time (r = 0.45, p < 0.001). They immediately conclude that more console time causes better outcomes. What's wrong with this reasoning?

- a) Nothing—the correlation is strong enough to imply causation
- b) They haven't checked if this finding is robust to controls
- c) They're making a causal claim from correlational data—residents who get more time may already be better
- d) The sample size is probably too small

**Correct**: c
**Why**: This is a classic selection problem. Better residents may get more console time BECAUSE they're already good, not the other way around. Causal claims require ruling out reverse causality.

---

### Question 5
You notice that some surgical residents learn much faster than others despite similar formal training. Which pattern is most worth pursuing?

- a) The average learning curve across all residents
- b) What successful learners do differently—the mechanisms behind their success
- c) Whether the difference is statistically significant
- d) How to control for individual differences in future studies

**Correct**: b
**Why**: The heterogeneity in outcomes signals that something interesting is happening. The practices that distinguish successful learners from average ones IS the story.

---

### Question 6
A researcher finds that "shadow learning" practices predict robotic surgical skill (β = 0.38, p < 0.01). Before writing up the finding, they should:

- a) Frame it against communities of practice theory
- b) Check if the effect survives controlling for prior experience, training site, and specialty
- c) Find more vivid quotes to illustrate the pattern
- d) Calculate the practical significance

**Correct**: b
**Why**: The effect could be spurious. Residents who engage in shadow learning might differ in other ways (more motivated, better mentors, more competitive programs). Robustness checks come before framing.

---

### Question 7
Your initial hypothesis was that robot adoption improves surgical outcomes. The data shows outcomes improve for complex cases but worsen for simple cases. What do you do?

- a) Report the null average effect with this heterogeneity as an interesting note
- b) Report only the complex cases where the hypothesis worked
- c) Investigate the heterogeneity—why does the robot help for complex cases but hurt for simple ones? This IS the paper
- d) Conclude that robot effects are "mixed"

**Correct**: c
**Why**: The heterogeneity is often more interesting than any main effect. Understanding WHY the robot helps in some contexts but not others could be a major contribution.

---

### Question 8
A researcher finds a significant effect (p = 0.03) but the effect size is tiny (d = 0.08). They are excited to publish. What should you tell them?

- a) Significance is what matters—publish it
- b) The effect is too small to matter practically—this finding should probably be killed
- c) Run more studies to increase the effect size
- d) Just report the p-value without the effect size

**Correct**: b
**Why**: Statistical significance ≠ substantive importance. An effect of d = 0.08 is practically meaningless even if p < 0.05. Finding discipline means killing findings that don't matter.

---

### Question 9
During data exploration, you find an unexpected pattern: nurses predict surgical complications better than surgeons' pre-operative assessments. This was not your research question. What should you do?

- a) Ignore it—it's not what you set out to study
- b) Note it but don't pursue it—scope creep is dangerous
- c) Investigate it—unexpected findings that challenge existing beliefs are often paper-worthy
- d) Ask your advisor what to do

**Correct**: c
**Why**: The best findings often emerge from unexpected patterns. If nurses have information surgeons don't, that's theoretically significant and practically important.

---

### Question 10
You've tested 15 different specifications of a model. One shows a significant effect (p = 0.04). What's the appropriate response?

- a) Report the significant one—that's the "true" specification
- b) This is likely a false positive from multiple testing—treat with extreme skepticism
- c) Report all 15 and let readers decide
- d) Choose the specification with the best theoretical justification, regardless of significance

**Correct**: b
**Why**: With 15 tests at α = 0.05, you'd expect ~0.75 false positives by chance. One significant finding out of 15 is consistent with noise. Robustness instinct means recognizing when patterns are likely spurious.

---

## Section B: Short Answer (2 questions)

### Question 11
A colleague finds that surgical trainees report high satisfaction with robotic training (mean = 4.2/5). But your observational data shows they spend <5% of procedure time actually operating the robot. How would you reconcile these findings, and what does this tell us about pattern recognition?

**Model answer should include**:
- Recognition that stated preferences ≠ revealed preferences
- The "satisfaction" may reflect other aspects (prestige, technology interest) not actual skill development
- The behavioral data (time on task) is likely more valid for assessing learning opportunities
- This disconnect itself could be theoretically interesting—why would trainees report satisfaction with training that isn't providing hands-on experience?

---

### Question 12
Describe a situation where you should KILL a finding rather than pursue it. What are the signs that a pattern isn't worth investigating further?

**Model answer should include any 3 of**:
- Effect doesn't survive obvious robustness checks
- Effect size is too small to matter practically
- Pattern is well-established in prior literature (replication, not contribution)
- Effect is likely driven by data artifacts or selection
- No theoretical mechanism explains the pattern
- Pattern doesn't hold in subgroups where it should be strongest
- Multiple specifications fail; only one works (likely false positive)

---

## Answer Key

| Question | Answer |
|----------|--------|
| 1 | b |
| 2 | b |
| 3 | c |
| 4 | c |
| 5 | b |
| 6 | b |
| 7 | c |
| 8 | b |
| 9 | c |
| 10 | b |
| 11 | See model answer |
| 12 | See model answer |

**Scoring**:
- MC: 1 point each (10 points)
- Short answer: 2 points each (4 points)
- Total: 14 points
- Pass: ≥11 points (80%)
