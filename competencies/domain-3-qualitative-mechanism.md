# Domain 3: Qualitative Mechanism Extraction

## What This Domain Tests

The ability to systematically extract mechanism evidence from qualitative data—not just reading and summarizing, but hypothesis-driven coding that finds supporting AND disconfirming evidence.

**Source in TheoryForge**: `/mine-qual`

## Why It Matters

This is perhaps THE most critical domain for mixed-methods researchers. Qualitative data shows you WHY and HOW—the mechanisms behind quantitative patterns. But novice researchers often:
- Read data aimlessly ("tell me what's interesting")
- Find only what they want to find
- Never look for disconfirming evidence
- Select quotes that illustrate rather than illuminate

Expert qualitative researchers enter data with hypotheses, code systematically, and actively hunt for evidence that challenges their interpretations.

## Core Competencies

### 3.1 Hypothesis Generation Before Reading
Forming expectations before seeing the data—to know what you're looking for and catch yourself when biased.

**What it looks like**:
- "If [mechanism X] is operating, I should see evidence of [specific behavior/attitude]"
- "Workers in [situation A] should talk about this differently than workers in [situation B]"
- "If my interpretation is right, informants should describe [specific experience]"

**Common failures**:
- Reading data with no expectations (results in aimless extraction)
- "Let me see what's interesting" (code for "I'll find what I want")
- Never writing down predictions (can't check for confirmation bias)

### 3.2 Systematic Coding (Not Open Exploration)
Having a coding scheme and applying it consistently across all data.

**What it looks like**:
- "For each interview, I'm coding for these 5 mechanisms"
- "A quote supports hypothesis X if it shows [specific criteria]"
- "I've coded all interviews; here's the distribution of evidence"

**Common failures**:
- Reading until you find good quotes, then stopping
- Coding some interviews but not others
- Changing criteria mid-stream without documenting
- Only reading interviews from informants you expect to support your view

### 3.3 Disconfirming Evidence Hunting
Actively seeking quotes and cases that challenge your interpretation.

**What it looks like**:
- "2 of 15 informants describe the opposite experience—here's what they said"
- "Site B workers don't show the mechanism—that's a boundary condition"
- "This quote directly challenges my interpretation: [quote]"

**Common failures**:
- Zero disconfirming evidence reported (impossible if looking honestly)
- Disconfirming evidence exists but isn't mentioned
- Treating disconfirming evidence as "noise" to explain away

### 3.4 Quote Selection Discipline
Choosing quotes that illuminate mechanisms, not just illustrate conclusions.

**What it looks like**:
- "This quote shows the mechanism operating in real-time"
- "This quote reveals heterogeneity—why some respond differently"
- "This quote is from a credible informant with direct experience"

**Common failures**:
- Selecting quotes because they're vivid, not because they're informative
- All quotes from same informant or site
- Quotes that say what you want but from informants without relevant experience
- Using quotes as decoration rather than evidence

---

## Assessment Specifications

### Foundation (Level 1)

**Format**: Multiple choice + scenario analysis

**Sample questions**:

1. Before reading interview data, a researcher should:
   - a) Have an open mind and see what emerges
   - b) Generate specific hypotheses about what mechanisms might operate
   - c) Know exactly what they'll find
   - d) Read the literature more

   **Correct**: b. Hypotheses create searchable predictions; "open mind" often means confirmation bias.

2. Which coding approach is most appropriate for mechanism extraction?
   - a) Read each interview and note interesting themes
   - b) Code all interviews against a pre-defined set of mechanism hypotheses
   - c) Find 5-10 good quotes that support the main finding
   - d) Have a research assistant summarize key points

   **Correct**: b. Systematic coding with explicit criteria.

3. A researcher finds that 12 of 15 informants support the proposed mechanism, but 3 describe the opposite experience. What should they do?
   - a) Report only the 12 supporting informants (they're the majority)
   - b) Investigate why the 3 differ—they might reveal a boundary condition
   - c) Conclude the mechanism is only partially supported
   - d) Recode the 3 interviews more carefully to find supporting evidence

   **Correct**: b. The exceptions often reveal the most interesting theoretical insight.

4. Which quote selection demonstrates good practice?
   - a) "I chose the most eloquent quotes from our most senior informants"
   - b) "I selected quotes where the mechanism is visible in action, including one that challenges my interpretation"
   - c) "I found quotes that perfectly match my hypotheses"
   - d) "I included memorable quotes that will engage readers"

   **Correct**: b. Illumination over illustration; includes disconfirming evidence.

**Passing threshold**: ≥80%

---

### Practice (Level 2)

**Format**: Code a subset of interviews against provided hypotheses

**Materials provided**:
- A robust quantitative pattern (e.g., "Workers with X show higher Y")
- 5-10 interview transcripts (anonymized, ~20-30 pages total)
- The theory being extended and sensitizing literature
- Pre-specified mechanism hypotheses to test

**Task**:
1. **Before reading**: Write your predictions—which hypotheses will have strong support? Weak support? What would surprise you?

2. **Code interviews**: For each mechanism hypothesis:
   - Extract supporting quotes (with informant ID, context)
   - Extract challenging quotes (with informant ID, context)
   - Note unexpected themes

3. **Report**:
   - Which mechanisms have strongest evidence?
   - What disconfirming evidence exists?
   - What did you find that wasn't in the hypotheses?
   - Which quotes would you use in a paper, and why?

**Evaluation rubric**:

| Criterion | Expert behavior | Points |
|-----------|-----------------|--------|
| Wrote predictions before reading | Pre-commitment discipline | 15 |
| Found ≥70% of expert-identified supporting quotes | Sees what's there | 20 |
| Found ≥50% of expert-identified disconfirming evidence | Doesn't cherry-pick | 25 |
| Coded all interviews systematically | Rigor and consistency | 20 |
| Selected quotes that illuminate (not illustrate) | Good judgment | 20 |

**Passing threshold**: ≥70 points; must achieve ≥15 on disconfirming evidence

**Feedback provided**:
- Expert's coding compared to student's
- Quotes student missed and why they matter
- Disconfirming evidence student didn't find
- Analysis of quote selection quality

---

### Mastery (Level 3)

**Format**: Independent coding on unfamiliar dataset

**Materials provided**:
- New quantitative pattern
- New interview dataset (10-15 interviews, ~40-50 pages)
- Theory context
- NO pre-specified hypotheses (student must generate)

**Task**:
1. Generate 5-7 mechanism hypotheses before reading
2. Code all interviews systematically
3. Report findings with evidence
4. Explicitly document disconfirming evidence
5. Select 10-15 placeable quotes with rationale

**Evaluation criteria**:

1. **Hypothesis Quality** (20 points)
   - Did they generate reasonable hypotheses?
   - Are hypotheses specific enough to test?
   - Do hypotheses connect to the theoretical framework?

2. **Coding Coverage** (25 points)
   - Were all interviews coded?
   - Was coding systematic (not selective)?
   - Did they find what experts found?

3. **Disconfirming Evidence** (30 points)
   - Did they find the disconfirming evidence that exists?
   - Is it documented honestly (not explained away)?
   - Did they identify boundary conditions?

4. **Quote Selection** (25 points)
   - Do quotes illuminate mechanisms?
   - Are quotes from appropriate informants?
   - Is there diversity (not all from one source)?
   - Is disconfirming evidence represented?

**Passing threshold**: ≥70 points total; must achieve ≥20 on Disconfirming Evidence

---

## What This Unlocks

| Level Achieved | TheoryForge Commands Unlocked |
|----------------|------------------------------|
| Level 2 | `/mine-qual` (must code 3 interviews manually first) |
| Level 3 | `/mine-qual` (full mode) |

**Prerequisite**: Must have passed Domain 2, Level 3

---

## The Mechanism Extraction Protocol

This is what TheoryForge's `/mine-qual` command automates—and what you need to understand to supervise it:

### Step 1: Generate Hypotheses
"If [mechanism] is operating, I should see evidence of [specific behavior/attitude/experience]"

### Step 2: Define Coding Criteria
For each hypothesis: What quotes would support it? Challenge it? What context is needed?

### Step 3: Code Systematically
Every interview, same criteria, document everything

### Step 4: Hunt Disconfirming Evidence
Actively search: "Who experienced the opposite? Where does the mechanism NOT operate?"

### Step 5: Assess Evidence Distribution
How many informants support vs challenge? Are supporters from one site/role/time? Is challenging evidence concentrated somewhere?

### Step 6: Select Placeable Quotes
Quotes that illuminate (show mechanism in action), from credible informants, with diversity

---

## Common Misconceptions

**"Let me see what emerges"**
This sounds open-minded but usually means confirmation bias. Enter with hypotheses.

**"I couldn't find disconfirming evidence"**
If you looked honestly, you'd find something. No mechanism operates universally. Zero disconfirming evidence is a red flag for cherry-picking.

**"The quote is vivid, so it's good"**
Vivid and informative are different. A quote can be memorable but not actually show the mechanism.

**"I coded until I had enough quotes"**
You should code ALL the data, not stop when you have what you need. Selection bias is the enemy.

**"Disconfirming evidence weakens my paper"**
Actually, documenting it honestly strengthens the paper. Reviewers trust researchers who acknowledge limitations.

---

## Readings & Resources

### Required
- TheoryForge `/mine-qual.md` — Full command spec
- Your field's leading qualitative methods papers (note how they handle disconfirming evidence)
- One "exemplary" qual paper from your advisor's recommendations

### Recommended
- Miles, Huberman, & Saldaña, "Qualitative Data Analysis" (selected chapters)
- Locke, "Grounded Theory in Management Research" (on theoretical sensitivity)
- Pratt (2009), "From the Editors: For the Lack of a Boilerplate" (AMJ)

---

## Self-Assessment Questions

Before taking the Level 3 assessment, you should be able to answer:

1. How do you generate mechanism hypotheses before reading data?
2. What's your coding protocol? How do you ensure consistency?
3. How do you actively search for disconfirming evidence?
4. What criteria do you use for quote selection?
5. How would you know if you were cherry-picking?

This domain has the strictest requirements because qualitative coding is where deep understanding develops. If you can't do this well, you can't supervise AI doing it.
